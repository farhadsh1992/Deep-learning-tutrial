{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:atuo;text-align:center;border: 4px solid black;background-color:#FF8000;color:white;border-radius: 25px\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:4px 4px 4px 4px;padding:0.3%;margin:19px;background-image: url(https://www.tensorflow.org/images/tf_logo_social.png);background-repeat: no-repeat;background-size: 320px\"> \n",
    "<header style=\"width:100%;height:auto;\">\n",
    "  <font size=\"23px\"><b> Chapter 00</b></font>\n",
    "    <h2> Tensorflow_tutorial </h2>\n",
    "    <h4> </h4>\n",
    "</header>\n",
    "\n",
    "<div><div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #FF8000;padding:9px;'>\n",
    "<div style=\"box-shadow: 0 8px 12px 0 white, 0 6px 20px 0 rgba(1, 1, 1, 1);padding:1%;margin:6px;border-radius: 25px\"> \n",
    "    \n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px;box-shadow: 0 1px 1px 0 #00264D, 0 6px 20px 0 rgba(1, 1, 1, 1)\">\n",
    "<div style=\"box-shadow: 0 1px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px;width:10%;text-align:center\">    \n",
    "  <strong> Refrence: </strong><br></div>  \n",
    "<div style=\"box-shadow: 0 1px 12px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px;margin-top:8px\">      \n",
    "\n",
    "    \n",
    "+ https://leepaultech.wordpress.com/2017/10/18/word-vector/    \n",
    "    \n",
    "+ https://www.youtube.com/watch?time_continue=22&v=PicxU81owCs    \n",
    "    \n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#CC8800;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px\">       \n",
    "  <strong> Summary: </strong><br>\n",
    "    \n",
    "        \n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:25%;height:30px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h5> contents-Paper:  </h5>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#FF8000;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#1\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> Word2vec </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:black;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\">  Read_file </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:black;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> word2vec code </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:black;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\"> write pakage by tensorflow:  </a></h6>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:white;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\">  </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:white;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\">   </a></h6>\n",
    "</div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1\" style=\"width:100%;height:atuo;border: 4px solid black;background-color:#FF8000;color:black;text-align:center;border-radius: 25px;padding:3px\">\n",
    "<div style=\"box-shadow: 0 1px 4px 0 #00264D, 0 60px 20px 0 rgba(1, 1, 1, 1);padding:0.5%;margin:6px;border-radius: 25px\"> \n",
    "           \n",
    "<h4><b> Word2vec </b></h4>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px\">    \n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "\n",
    "+ https://web.stanford.edu/class/cs20si/2017/lectures/notes_04.pdf\n",
    "    \n",
    "\n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#CC8800;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px\">       \n",
    "  <strong> Summary: </strong><br>\n",
    "    \n",
    "        \n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;border-radius: 15px\">\n",
    "\n",
    "```python\n",
    "tf.truncated_normal(shape,mean, stddev )\n",
    "```\n",
    "\n",
    "The generated values follow a normal distribution with specified mean and standard deviation    \n",
    "<hr>\n",
    "    \n",
    "http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf\n",
    "\n",
    "https://www.tensorflow.org/extras/candidate_sampling.pdf    \n",
    "```python\n",
    "# loss function (Noise Contrastive Estimation (NCE))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights, biases=biases , labels=word2vec, inputs= embed , num_sampled = NUM_SAMPLED,num_classes = VOCAB_SIZE))\n",
    "```    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\"  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:98%;height:28px;border: 4px solid black;background-color:black;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 grey, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:5px 5px 5px 5px;\">\n",
    "    <h5>  Read_file </h5>\n",
    "</div>\n",
    "   \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_reader_ops(filename_queue):\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, csv_row = reader.read(filename_queue)\n",
    "    record_defaults = [[\"\"], [\"\"], [0], [0], [0], [0]]\n",
    "    country, code, gold, silver, bronze, total = tf.decode_csv(csv_row, record_defaults=record_defaults)\n",
    "    features = tf.pack([gold, silver, bronze])\n",
    "    return features, country\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0])).split()\n",
    "    return data\n",
    "\n",
    "def read_csv_keras(filename, path, batch_size, bacK_dataset=True):\n",
    "    \"\"\"\n",
    "    https://www.youtube.com/watch?v=aKLcvGdOP6g\n",
    "    \"\"\"\n",
    "    csv = tf.keras.utils.get_file(fname=path, origin=filename)\n",
    "    \n",
    "    columns_name = []\n",
    "    feature = columns_name[:-1]\n",
    "    labels = columns_name[-1]\n",
    "    \n",
    "\n",
    "    dataset = tf.contrib.data.make_csv_dataset(csv, batch_size, column_names=feature,labels=labels, num_epochs=1 )\n",
    "    \n",
    "    feature, label = next(iter(dataset))\n",
    "    if bacK_dataset:\n",
    "        return dataset\n",
    "    else:\n",
    "        return feature, label\n",
    "    \n",
    "def BEST_READ_TF():\n",
    "    \"\"\"\n",
    "    https://www.tensorflow.org/alpha/tutorials/load_data/csv\n",
    "    \"\"\"\n",
    "    TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "    TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "    train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "    test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)\n",
    "    \n",
    "    \n",
    "    # CSV columns in the input file.\n",
    "    with open(train_file_path, 'r') as f:\n",
    "        names_row = f.readline()\n",
    "\n",
    "\n",
    "    CSV_COLUMNS = names_row.rstrip('\\n').split(',')\n",
    "    print(CSV_COLUMNS)\n",
    "    dataset = tf.contrib.data.make_csv_dataset(train_file_path, batch_size, column_names=CSV_COLUMNS, num_epochs=1 )\n",
    "    return dataset, CSV_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\"  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:98%;height:28px;border: 4px solid black;background-color:black;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 grey, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:5px 5px 5px 5px;\">\n",
    "    <h5> word2vec  </h5>\n",
    "</div>\n",
    "   \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = None # or BATCH_SIZE\n",
    "index_words = tf.placeholder(shape=[VOCAB_SIZE], dtype=tf.float32) # CAN BE ONE-HOT VECTOR, TOO\n",
    "word2vec = tf.placeholder(shape=[VOCAB_SIZE], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10\n",
    "EMBED_SIZE = 3\n",
    "embed_matrix = tf.Variable(tf.random_uniform(shape=[VOCAB_SIZE, EMBED_SIZE], minval = -1.0, maxval=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = tf.nn.embedding_lookup(params=embed_matrix, ids=index_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN:\n",
    "weights = tf.Variable(tf.truncated_normal(shape=[VOCAB_SIZE,EMBED_SIZE],stddev=(1.0/EMBED_SIZE**0.5)))\n",
    "biases = tf.Variable(tf.zeros([VOCAB_SIZE]))\n",
    "\n",
    "# loss function (Noise Contrastive Estimation (NCE))\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights, biases=biases , labels=word2vec, inputs= embed , num_sampled = NUM_SAMPLED,num_classes = VOCAB_SIZE))\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session as sess:\n",
    "    sess.run(tf.global_variables_initializer)\n",
    "    \n",
    "    avarge_loss = 0.0\n",
    "    for index in xrange(NUM_TRAIN_STEPS):\n",
    "        batch = batch_gen.next()\n",
    "        loss_batch, _ = sess.run([loss, optimizer], feed_dict={index_words: batch[0], word2vec: batch[1]})\n",
    "        \n",
    "        average_loss += loss_batch\n",
    "        if (index + 1) % 2000 == 0:\n",
    "            print('Average loss at step {}: {:5.1f}'.format(index + 1, average_loss / (index +1)))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\"  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:30%;height:28px;border: 4px solid black;background-color:black;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 grey, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:5px 5px 5px 5px;\">\n",
    "    <h5> write pakage by tensorflow:  </h5>\n",
    "</div>\n",
    "   \n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile farhad_word2vefc.py\n",
    "\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "class Word2Vec():\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.data = ''\n",
    "        self.LOGDIR=\n",
    "    def read_data(self,filename):\n",
    "        pass\n",
    "    def made_dictionary():\n",
    "            pass\n",
    "      \n",
    "    def (self,embed_size):\n",
    "        VOCAB_SIZE = made_dictionary.shape[1]\n",
    "        EMBED_SIZE = embed_size\n",
    "        \n",
    "        with tf.name_scope('data'):\n",
    "            index_words = tf.placeholder(shape=[VOCAB_SIZE], dtype=tf.float32, name=\"index\")\n",
    "            word2vec = tf.placeholder(shape=[VOCAB_SIZE], dtype=tf.float32, name=\"word2vec\" )\n",
    "            \n",
    "            tf.summary.histogram('index_words', index_words)\n",
    "            tf.summary.histogram('word2vec',word2vec)\n",
    "            \n",
    "        with tf.device('/cpu:0'):\n",
    "            with tf.name_scope('embed'):\n",
    "                embed_matrix = tf.Variable(tf.random_uniform(shape=[VOCAB_SIZE, EMBED_SIZE], minval = -1.0, maxval=1.0), name='embed_matrix')\n",
    "                embed = tf.nn.embedding_lookup(params=embed_matrix, ids=index_words)\n",
    "                \n",
    "                tf.summary.histogram('embed_matrix',embed_matrix)\n",
    "                tf.summary.histogram('embed',embed)\n",
    "            with tf.name_scope('weight_baise'):\n",
    "                weights = tf.Variable(tf.truncated_normal(shape=[VOCAB_SIZE,EMBED_SIZE],stddev=(1.0/EMBED_SIZE**0.5)), name='weights')\n",
    "                biases = tf.Variable(tf.zeros([VOCAB_SIZE]), name='biases')\n",
    "                tf.summary.histogram('weights',weights)\n",
    "                tf.summary.histogram('biases',biases)\n",
    "            \n",
    "            with tf.name_scope('loss'):\n",
    "                loss = tf.reduce_mean(tf.nn.nce_loss(weights=weights, biases=biases , labels=word2vec, inputs= embed , num_sampled = NUM_SAMPLED,num_classes = VOCAB_SIZE))\n",
    "                tf.summary.scalar('loss',loss)\n",
    "            with tf.name_scope('optimize'):\n",
    "                optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "                \n",
    "            summ = tf.summary.merge_all()  \n",
    "            writer = tf.summary.FileWriter(self.LOGDIR)\n",
    "            \n",
    "            with tf.Session as sess:\n",
    "            sess.run(tf.global_variables_initializer)\n",
    "    \n",
    "            avarge_loss = 0.0\n",
    "            for index in xrange(NUM_TRAIN_STEPS):\n",
    "                batch = batch_gen.next()\n",
    "                loss_batch, _ = sess.run([loss, optimizer], feed_dict={index_words: batch[0], word2vec: batch[1]})\n",
    "                writer.add_summary(loss_batch, index)\n",
    "                average_loss += loss_batch\n",
    "                if (index + 1) % 2000 == 0:\n",
    "                    print('Average loss at step {}: {:5.1f}'.format(index + 1, average_loss / (index +1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "<div style=\"box-shadow: 0 4px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;border-radius:35px\">\n",
    "\n",
    "## write code in terminal    \n",
    "```python\n",
    ">> tensorboard --logdir [ address ]\n",
    "```\n",
    "<br>\n",
    "<font color='red'> \n",
    "    \n",
    "Address = http://localhost:6006   </font>  \n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = '/Users/apple/Documents/Programming/pyfile/DataBase/Collection/Collected_Tweets/201949collection_tweets_about_apple.csv'\n",
    "name = '201949collection_tweets_about_apple.csv'\n",
    "data = create_file_reader_ops(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.insert>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
