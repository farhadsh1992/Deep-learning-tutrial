{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:atuo;text-align:center;border: 4px solid black;background-color:#FF8000;color:white;border-radius: 25px\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:4px 4px 4px 4px;padding:0.3%;margin:19px;background-image: url(https://www.tensorflow.org/images/tf_logo_social.png);background-repeat: no-repeat;background-size: 324px\"> \n",
    "<header style=\"width:100%;height:auto;\">\n",
    "  <font size=\"23px\"><b> Chapter 006</b></font>\n",
    "    <h2> Tensorflow_tutorial </h2>\n",
    "    <h4> Mnist,Convolutional Neural Networks </h4>\n",
    "</header>\n",
    "\n",
    "<div><div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #FF8000;padding:9px;'>\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px\"> \n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px;box-shadow: 0 1px 1px 0 #00264D, 0 6px 20px 0 rgba(1, 1, 1, 1)\">\n",
    "<div style=\"box-shadow: 0 1px 4px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px;width:10%;text-align:center\">    \n",
    "  <strong> Refrence: </strong><br></div>  \n",
    "<div style=\"box-shadow: 0 1px 12px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px;margin-top:8px\">      \n",
    "\n",
    "+ https://www.youtube.com/watch?v=xec6ResJCtk\n",
    "    \n",
    "+ https://www.youtube.com/watch?v=61mqENQgbV8   \n",
    "    \n",
    "+ https://www.tensorflow.org/tutorials/images/deep_cnn    \n",
    "    \n",
    "+ https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/contrib/eager/python/examples/pix2pix/pix2pix_eager.ipynb    \n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:25%;height:30px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h5> contents-Paper:  </h5>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;height:45px\">\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:0 ;border: 4px solid black;background-color:#FF8000;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\">  </a></h6>\n",
    "</div>\n",
    "<div style=\"width:49%;height:30px;position:absolute;left:50% ;border: 4px solid black;background-color:#FF8000;color:black;text-align:center;border-radius:25px 25px 25px 25px;padding:3px\">\n",
    "    <h6><a href=\"#\" style=\"position: relative;padding:5px;color:white;text-align: center;\">   </a></h6>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\" style=\"width:100%;height:70px;border: 4px solid black;background-color:#FF8000;color:black;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h3>    </h3>\n",
    "    <h5>     </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:50%;height:40px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 25px 25px 0px;padding:3px\">\n",
    "    <h4> :  </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\" style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:5%;height:28px;border: 4px solid black;background-color:#FF8000;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #FF8000, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:6.5%;box-shadow: 0 4px 8px 0 #990000, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5> Abstract: </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\" style=\"width:100%;height:40px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>   </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\" style=\"width:100%;height:40px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h4>   </h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%\">\n",
    "     \n",
    "    \n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\"  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#990000;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #990000, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "<div style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#FFFF00;color:black;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 #FFFF00, 0 6px 20px 0 rgba(0, 0, 0, 0.19);\">\n",
    "    <h5>  : </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:10%;height:28px;border: 4px solid black;background-color:#FF8000;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 #00404D, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "<div id=\"\"  style=\"position:absolute;width:40%;height:28px;border: 4px solid black;background-color:#1E00B3;color:white;text-align:center;border-radius:0px 45px 45px 0px;padding:3px;left:11.5%;box-shadow: 0 4px 8px 0 blue, 0 6px 20px 0 rgba(0, 0, 0, 0.19)\">\n",
    "    <h5> : </h5>\n",
    "</div>    \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\"  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:30%;height:28px;border: 4px solid black;background-color:black;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 grey, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:0 25px 25px 0;\">\n",
    "    <h5> Abstract  </h5>\n",
    "</div>\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"\"  style=\"height:43px\">\n",
    "<div style=\"position:absolute;width:98%;height:28px;border: 4px solid black;background-color:black;color:white;text-align:center;border-radius:0px 0px 0px 0px;padding:3px;box-shadow: 0 4px 8px 0 grey, 0 6px 20px 0 rgba(0, 0, 0, 0.19);border-radius:5px 5px 5px 5px;\">\n",
    "    <h5>   </h5>\n",
    "</div>\n",
    "   \n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:70%;border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%\">\n",
    "<div style=\"height:154px\">\n",
    "<img src=\"http://juditacs.github.io/assets/padded_sequence.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "<img src=\"http://juditacs.github.io/assets/softmax_before_after.png\" style=\"position:absoluate;height:150px;border:3px solid #1E00B3\">\n",
    "   \n",
    "</div>\n",
    "\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:22%;text-align:center;position:absolute;left:1.3%;border:3px solid #1E00B3\"><b>Mask(or Padding)</b></div> \n",
    "<div style=\"width:22.2%;text-align:center;position:absolute;left:24.1%;border:3px solid #1E00B3\"><b>Whithout mask</b></div>\n",
    "<div style=\"width:21.2%;text-align:center;position:absolute;left:47.1%;border:3px solid #1E00B3\"><b>with mask</b></div>     \n",
    "   \n",
    "</div>\n",
    "<div style=\"height:30px\">\n",
    "<div style=\"width:67%;text-align:center;position:absolute;left:1.3%;border:3px solid #00264D\"><b>fig24</b></div>     \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"1\" style=\"width:100%;height:atuo;border: 4px solid black;background-color:#FF8000;color:black;text-align:center;border-radius: 25px;padding:3px\">\n",
    "<div style=\"box-shadow: 0 1px 4px 0 #00264D, 0 60px 20px 0 rgba(1, 1, 1, 1);padding:0.5%;margin:6px;border-radius: 25px\"> \n",
    "           \n",
    "<h4><b>   </b></h4>\n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px;box-shadow: 0 1px 1px 0 #00264D, 0 6px 20px 0 rgba(1, 1, 1, 1)\">\n",
    "<div style=\"box-shadow: 0 1px 4px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px;width:10%;text-align:center\">    \n",
    "  <strong> Refrence: </strong><br></div>  \n",
    "<div style=\"box-shadow: 0 1px 12px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px;margin-top:8px\">      \n",
    "\n",
    "+ https://www.youtube.com/watch?v=xec6ResJCtk\n",
    "    \n",
    "+ https://www.youtube.com/watch?v=61mqENQgbV8   \n",
    "    \n",
    "+ https://www.tensorflow.org/tutorials/estimators/cnn    \n",
    "    \n",
    "</div>    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #4D0033;background-color:#CC8800;color:black;border-radius: 5px;padding:7px;color:white;\">\n",
    "<div style=\"box-shadow: 0 14px 18px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:1%;margin:6px;border-radius: 25px\">       \n",
    "  <strong> Summary: </strong><br>\n",
    "    \n",
    "        \n",
    "</div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from farhad.utility import encode_text_dummy_array\n",
    "from farhad.time_estimate import EstimateFaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = fashion_mnist().load_data() \n",
    "path_tb = 'log/5/'\n",
    "train, test = mnist.load_data()\n",
    "x_train, y_train= train \n",
    "x_test, y_test = test\n",
    "\n",
    "x_train, x_test= x_train/255.0, x_test/255.0 \n",
    "y_train, y_test =  encode_text_dummy_array(y_train).values, encode_text_dummy_array(y_test).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = tf.data.Dataset.from_tensors(x_train), tf.data.Dataset.from_tensors(x_test)\n",
    "y_train, y_test  = tf.data.Dataset.from_tensors(y_train), tf.data.Dataset.from_tensors(y_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train, y_test =  np.float(encode_text_dummy_array(y_train)), np.float(encode_text_dummy_array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-58344646dc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,data):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        self._data = data\n",
    "        self._num_examples = data.shape[0]\n",
    "        pass\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    def next_batch(self,batch_size,shuffle = True):\n",
    "        start = self._index_in_epoch\n",
    "        if start == 0 and self._epochs_completed == 0:\n",
    "            idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx)  # shuffle indexe\n",
    "            self._data = self.data[idx]  # get list of `num` random samples\n",
    "\n",
    "        # go to the next batch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            self._epochs_completed += 1\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            data_rest_part = self.data[start:self._num_examples]\n",
    "            idx0 = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx0)  # shuffle indexes\n",
    "            self._data = self.data[idx0]  # get list of `num` random samples\n",
    "\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples #avoid the case where the #sample != integar times of batch_size\n",
    "            end =  self._index_in_epoch  \n",
    "            data_new_part =  self._data[start:end]  \n",
    "            return np.concatenate((data_rest_part, data_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._data[start:end]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(inputs, channel_in, channel_out,name):\n",
    "    #\n",
    "    with tf.name_scope('conv2D_{}'.format(name)):\n",
    "        w = tf.Variable(tf.zeros(shape=[3,3,channel_in,channel_out]),name='w_conv')\n",
    "        b = tf.Variable(tf.zeros(shape=[channel_out]), name='b_conv')\n",
    "        conv2 = tf.nn.conv2d(inputs, w, strides=[1,1,1,1], padding=\"SAME\", name='conv2d_conv')\n",
    "        act = tf.nn.relu(conv2+b)\n",
    "        \n",
    "        tf.summary.histogram(path_tb+\"w_conv\"+name, w)\n",
    "        tf.summary.histogram(path_tb+\"b_conv\"+name,b)\n",
    "        tf.summary.histogram(path_tb+\"act_conv\"+name,act)\n",
    "        \n",
    "        return act\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(inputs, units , name):\n",
    "    with tf.name_scope('feedforeard{}'.format(name)):\n",
    "        w = tf.Variable(tf.zeros(shape=[inputs.shape[1], units]),name='w_ff')\n",
    "        b = tf.Variable(tf.zeros(shape=[units]),name='b_ff')\n",
    "        ma = tf.matmul(inputs,w)\n",
    "        act = tf.nn.relu(ma+b, name=\"act_ff\")\n",
    "        \n",
    "        tf.summary.histogram(path_tb+\"w_ff\"+name, w)\n",
    "        tf.summary.histogram(path_tb+\"b_ff\"+name,b)\n",
    "        tf.summary.histogram(path_tb+\"act_ff\"+name,act)\n",
    "        \n",
    "        return act\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_graph(sess, name):\n",
    "    join = os.path.join(path_tb, name)\n",
    "    file_writer = tf.summary.FileWriter(join)\n",
    "    file_writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnist_model():\n",
    "    def __init__(self,intputs):\n",
    "        self.sess =  tf.Session()\n",
    "        \n",
    "       \n",
    "        self.pred = ''\n",
    "        \n",
    "        self.cross_entropy= ''\n",
    "        self.train_step =''\n",
    "        self.accuracy = ''\n",
    "        self.summ =\"\"\n",
    "        self.writer = \"\"\n",
    "        self.intputs = intputs\n",
    "    \n",
    "        self.x = tf.placeholder(shape=[None,self.intputs.shape[1],self.intputs.shape[2]],dtype=tf.float32, name='as_inputs')\n",
    "        self.x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "        tf.summary.image(path_tb+'as_inputs',self.x, 3)\n",
    "    \n",
    "        self.y = tf.placeholder(shape=[None,10],dtype=tf.float32, name=\"as_outputs\")\n",
    "    \n",
    "        conv_1 = conv2d(inputs=self.x_image, channel_in=1, channel_out=32,name=\"conv1\")\n",
    "        pool_1 = tf.nn.max_pool(conv_1, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "        Flatten_shape = [-1, (14)*(14)*32] #14/2\n",
    "        Flatten_1 = tf.reshape(pool_1, shape=Flatten_shape , name=\"Flatten\")\n",
    "    \n",
    "        ff_1 = feedforward(Flatten_1, units=128, name=\"ff_1\")\n",
    "        ff_2 = feedforward(ff_1, units=10, name=\"ff_2\")\n",
    "        ff_3 = tf.nn.softmax(ff_2, name=\"softmax_layer\")\n",
    "    \n",
    "        tf.summary.histogram(path_tb+'softmax',ff_3)\n",
    "        self.pred = ff_3\n",
    "        \n",
    "    \n",
    "    def compiles(self,learning_rate=1e-4):\n",
    "        \n",
    "        with tf.name_scope('Losses_funcation'):\n",
    "            self.cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.pred, labels=self.y), name=\"cross_entropy\")\n",
    "            tf.summary.scalar(path_tb+'cross_entropy', self.cross_entropy)\n",
    "            \n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.cross_entropy)\n",
    "            \n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "            tf.summary.scalar(path_tb+\"accuracy\", self.accuracy)\n",
    "        \n",
    "        self.summ = tf.summary.merge_all() \n",
    "        self.saver = tf.train.Saver()        \n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        write_graph(self.sess, 'model')\n",
    "        \n",
    "    def make_hparam_string(learning_rate, use_two_fc=False, use_two_conv=False):\n",
    "        conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "        fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "        return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "    \n",
    "    def next_batch(self,num, data, labels):\n",
    "        '''\n",
    "        Return a total of `num` random samples and labels. \n",
    "        '''\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:num]\n",
    "        data_shuffle = [data[ i] for i in idx]\n",
    "        labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "        return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "    \n",
    "    def fit(self, inputs, outputs, validation_size=0.20 , epoch=1 ,verbose=0 , batch=30):\n",
    "        \n",
    "        data = inputs,outputs\n",
    "        X_train, X_test, y_train, y_test = train_test_split( inputs,outputs , test_size=validation_size)\n",
    "        X_train_1,  y_train_1 = Dataset(X_train), Dataset(y_train)\n",
    "        for i in range(epoch+1):\n",
    "            X_train,  y_train  = X_train_1.next_batch(batch),  y_train_1.next_batch(batch)\n",
    "            if i % 5 == 0:\n",
    "                [train_accuracy, s] = self.sess.run([self.accuracy, self.summ], feed_dict={self.x: X_train, self.y: y_train})\n",
    "                \n",
    "            if  verbose==1 and i % 5 == 0:\n",
    "                con = int((i+1)*50/epoch)\n",
    "                con_ant = 50-con\n",
    "                #run = \"[step:\"+str(i)+\"/\"+str(epoch)+\"|\"+\"|\"+\"[training accuracy:\"+str(train_accuracy)+\"] \\n\"\n",
    "                sys.stdout.write(\"[step: {}/{}]|{}{}|[training accuracy: {}] \\n\".format(i,epoch,con*'#',con_ant*' ',train_accuracy))\n",
    "                #sys.stdout.write(run)\n",
    "            if i % 10 == 0:\n",
    "                #self.sess.run(feed_dict={self.x: X_test, self.y: y_test}) #assignment, \n",
    "                self.saver.save(self.sess, os.path.join(path_tb, \"model.ckpt\"), i)\n",
    "                #write_graph(self.sess, str(i))\n",
    "                \n",
    "            self.sess.run(self.train_step, feed_dict={self.x: X_train, self.y: y_train})\n",
    "            try:\n",
    "                self.writer = tf.summary.FileWriter(path_tb + hparam)\n",
    "            except:\n",
    "                 self.writer = tf.summary.FileWriter(path_tb + 'model')\n",
    "            EstimateFaster(i,epoch,\"process is runing,training_accuracy:{}\".format(train_accuracy))\n",
    "        print('last accuracy:', train_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TensorDataset' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fd0b224a2590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0e8c67d535b3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, intputs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'as_inputs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_tb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'as_inputs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TensorDataset' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "epoch = 2000\n",
    "batch = 80\n",
    "\n",
    "model = mnist_model(x_train)\n",
    "model.compiles(learning_rate=learning_rate)\n",
    "model.fit(x_train,y_train, epoch=epoch, verbose=0, batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 5px solid #00264D;box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:0.6%;border-radius:15px\">\n",
    "<div style=\"box-shadow: 0 4px 8px 0 #00264D, 0 6px 20px 0 rgba(0, 0, 0, 0.19);padding:2%;border-radius:85px\">\n",
    "\n",
    "## write code in terminal    \n",
    "```python\n",
    ">> tensorboard --logdir  /Users/apple/Documents/Programming/pyfile/deep_learning_tutrial/Tensorflow/log/5\n",
    "```\n",
    "<br>\n",
    "<font color='red' size=\"5px\"> \n",
    "    \n",
    "Address = http://localhost:6006   </font>  \n",
    "    \n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting TF_tutrial_CNN_A.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile TF_tutrial_CNN_A.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist, mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from farhad.utility import encode_text_dummy_array\n",
    "from farhad.time_estimate import EstimateFaster\n",
    "\n",
    "import numpy as np \n",
    "path_tb = 'log/6/'\n",
    "def conv2d(inputs, channel_in, channel_out,name):\n",
    "    #\n",
    "    with tf.name_scope('conv2D_{}'.format(name)):\n",
    "        w = tf.Variable(tf.zeros(shape=[3,3,channel_in,channel_out]),name='w_conv')\n",
    "        b = tf.Variable(tf.zeros(shape=[channel_out]), name='b_conv')\n",
    "        conv2 = tf.nn.conv2d(inputs, w, strides=[1,1,1,1], padding=\"SAME\", name='conv2d_conv')\n",
    "        act = tf.nn.relu(conv2+b)\n",
    "        \n",
    "        tf.summary.histogram(path_tb+\"w_conv\"+name, w)\n",
    "        tf.summary.histogram(path_tb+\"b_conv\"+name,b)\n",
    "        tf.summary.histogram(path_tb+\"act_conv\"+name,act)\n",
    "        \n",
    "        return act\n",
    "    \n",
    "def feedforward(inputs, units , name):\n",
    "    with tf.name_scope('feedforeard{}'.format(name)):\n",
    "        w = tf.Variable(tf.zeros(shape=[inputs.shape[1], units]),name='w_ff')\n",
    "        b = tf.Variable(tf.zeros(shape=[units]),name='b_ff')\n",
    "        ma = tf.matmul(inputs,w)\n",
    "        act = tf.nn.relu(ma+b, name=\"act_ff\")\n",
    "        \n",
    "        tf.summary.histogram(path_tb+\"w_ff\"+name, w)\n",
    "        tf.summary.histogram(path_tb+\"b_ff\"+name,b)\n",
    "        tf.summary.histogram(path_tb+\"act_ff\"+name,act)\n",
    "        \n",
    "        return act\n",
    "            \n",
    "def write_graph(sess, name):\n",
    "    join = os.path.join(path_tb, name)\n",
    "    file_writer = tf.summary.FileWriter(join)\n",
    "    file_writer.add_graph(sess.graph)\n",
    "    \n",
    "class Dataset:\n",
    "    def __init__(self,data):\n",
    "        self._index_in_epoch = 0\n",
    "        self._epochs_completed = 0\n",
    "        self._data = data\n",
    "        self._num_examples = data.shape[0]\n",
    "        pass\n",
    "\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    def next_batch(self,batch_size,shuffle = True):\n",
    "        start = self._index_in_epoch\n",
    "        if start == 0 and self._epochs_completed == 0:\n",
    "            idx = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx)  # shuffle indexe\n",
    "            self._data = self.data[idx]  # get list of `num` random samples\n",
    "\n",
    "        # go to the next batch\n",
    "        if start + batch_size > self._num_examples:\n",
    "            self._epochs_completed += 1\n",
    "            rest_num_examples = self._num_examples - start\n",
    "            data_rest_part = self.data[start:self._num_examples]\n",
    "            idx0 = np.arange(0, self._num_examples)  # get all possible indexes\n",
    "            np.random.shuffle(idx0)  # shuffle indexes\n",
    "            self._data = self.data[idx0]  # get list of `num` random samples\n",
    "\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size - rest_num_examples #avoid the case where the #sample != integar times of batch_size\n",
    "            end =  self._index_in_epoch  \n",
    "            data_new_part =  self._data[start:end]  \n",
    "            return np.concatenate((data_rest_part, data_new_part), axis=0)\n",
    "        else:\n",
    "            self._index_in_epoch += batch_size\n",
    "            end = self._index_in_epoch\n",
    "            return self._data[start:end]\n",
    "\n",
    "\n",
    "class mnist_model():\n",
    "    def __init__(self,intputs):\n",
    "        self.sess =  tf.Session()\n",
    "        \n",
    "       \n",
    "        self.pred = ''\n",
    "        \n",
    "        self.cross_entropy= ''\n",
    "        self.train_step =''\n",
    "        self.accuracy = ''\n",
    "        self.summ =\"\"\n",
    "        self.writer = \"\"\n",
    "        self.intputs = intputs\n",
    "    \n",
    "        self.x = tf.placeholder(shape=[None,28,28],dtype=tf.float32, name='as_inputs')\n",
    "        self.x_image = tf.reshape(self.x, [-1,28,28,1])\n",
    "        tf.summary.image(path_tb+'as_inputs',self.x, 3)\n",
    "    \n",
    "        self.y = tf.placeholder(shape=[None,10],dtype=tf.float32, name=\"as_outputs\")\n",
    "    \n",
    "        conv_1 = conv2d(inputs=self.x_image, channel_in=1, channel_out=32,name=\"conv1\")\n",
    "        pool_1 = tf.nn.max_pool(conv_1, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")\n",
    "    \n",
    "        Flatten_shape = [-1, (14)*(14)*32] #14/2\n",
    "        Flatten_1 = tf.reshape(pool_1, shape=Flatten_shape , name=\"Flatten\")\n",
    "    \n",
    "        ff_1 = feedforward(Flatten_1, units=128, name=\"ff_1\")\n",
    "        ff_2 = feedforward(ff_1, units=10, name=\"ff_2\")\n",
    "        ff_3 = tf.nn.softmax(ff_2, name=\"softmax_layer\")\n",
    "    \n",
    "        tf.summary.histogram(path_tb+'softmax',ff_3)\n",
    "        self.pred = ff_3\n",
    "        \n",
    "    \n",
    "    def compiles(self,learning_rate=1e-4):\n",
    "        \n",
    "        with tf.name_scope('Losses_funcation'):\n",
    "            self.cross_entropy = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.pred, labels=self.y), name=\"cross_entropy\")\n",
    "            tf.summary.scalar(path_tb+'cross_entropy', self.cross_entropy)\n",
    "            \n",
    "        with tf.name_scope('optimizer'):\n",
    "            self.train_step = tf.train.AdamOptimizer(learning_rate).minimize(self.cross_entropy)\n",
    "            \n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(self.pred, 1), tf.argmax(self.y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))   \n",
    "            tf.summary.scalar(path_tb+\"accuracy\", self.accuracy)\n",
    "        \n",
    "        self.summ = tf.summary.merge_all() \n",
    "        self.saver = tf.train.Saver()        \n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        write_graph(self.sess, 'model')\n",
    "        \n",
    "    def make_hparam_string(learning_rate, use_two_fc=False, use_two_conv=False):\n",
    "        conv_param = \"conv=2\" if use_two_conv else \"conv=1\"\n",
    "        fc_param = \"fc=2\" if use_two_fc else \"fc=1\"\n",
    "        return \"lr_%.0E,%s,%s\" % (learning_rate, conv_param, fc_param)\n",
    "    \n",
    "    def next_batch(self,num, data, labels):\n",
    "        '''\n",
    "        Return a total of `num` random samples and labels. \n",
    "        '''\n",
    "        idx = np.arange(0 , len(data))\n",
    "        np.random.shuffle(idx)\n",
    "        idx = idx[:num]\n",
    "        data_shuffle = [data[ i] for i in idx]\n",
    "        labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "        return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "    \n",
    "    def fit(self, inputs, outputs, validation_size=0.20 , epoch=1 ,verbose=0 , batch=30):\n",
    "        \n",
    "        data = inputs,outputs\n",
    "        X_train, X_test, y_train, y_test = train_test_split( inputs,outputs , test_size=validation_size)\n",
    "        X_train_1,  y_train_1 = Dataset(X_train), Dataset(y_train)\n",
    "        for i in range(epoch+1):\n",
    "            X_train,  y_train  = X_train_1.next_batch(batch),  y_train_1.next_batch(batch)\n",
    "            if i % 5 == 0:\n",
    "                [train_accuracy, s] = self.sess.run([self.accuracy, self.summ], feed_dict={self.x: X_train, self.y: y_train})\n",
    "                \n",
    "            if  verbose==1 and i % 5 == 0:\n",
    "                con = int((i+1)*50/epoch)\n",
    "                con_ant = 50-con\n",
    "                #run = \"[step:\"+str(i)+\"/\"+str(epoch)+\"|\"+\"|\"+\"[training accuracy:\"+str(train_accuracy)+\"] \\n\"\n",
    "                sys.stdout.write(\"[step: {}/{}]|{}{}|[training accuracy: {}] \\n\".format(i,epoch,con*'#',con_ant*' ',train_accuracy))\n",
    "                #sys.stdout.write(run)\n",
    "            if i % 10 == 0:\n",
    "                #self.sess.run(feed_dict={self.x: X_test, self.y: y_test}) #assignment, \n",
    "                self.saver.save(self.sess, os.path.join(path_tb, \"model.ckpt\"), i)\n",
    "                #write_graph(self.sess, str(i))\n",
    "                \n",
    "            self.sess.run(self.train_step, feed_dict={self.x: X_train, self.y: y_train})\n",
    "            try:\n",
    "                self.writer = tf.summary.FileWriter(path_tb + hparam)\n",
    "            except:\n",
    "                 self.writer = tf.summary.FileWriter(path_tb + 'model')\n",
    "            EstimateFaster(i,epoch,\"process is runing,training_accuracy:{}\".format(train_accuracy))\n",
    "        print('last accuracy:', train_accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
