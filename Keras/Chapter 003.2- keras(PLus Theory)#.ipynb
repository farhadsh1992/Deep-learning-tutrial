{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:125px;text-align:center;border: 4px solid #179871;background-color:#179871;color:white\">\n",
    "\n",
    "<header style=\"width:100%;height:140px;\">\n",
    "  <h1>Chapter 2.2</h1>\n",
    "    <h1><b>+ PLus Theory 3</b></h1>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #179871;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brief contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div  style=\"width:300px;position:absolute;left: auto;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#Forward_propagation\" style=\"padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Forward propagation</b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:300px;position:absolute;left: 305px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#backward_propagation\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>backward propagation</b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 610px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>list of layers </b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 915px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>list of optimazer funcation  </b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 1220px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>list of losses funcation¶ </b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!------------------------------------------------------------------------------------------------------------------------>\n",
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div  style=\"width:300px;position:absolute;left: auto;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#layers\"style=\"padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Theory of layers</b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div  style=\"width:300px;position:absolute;left: 305px;border:4px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#optimazer_funcation\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Theory of optimazer funcation</b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 610px;border:4px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Theory of activations funcation </b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 915px;border:4px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#losses-funcation \"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Theory of losses funcation  </b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 1220px;border:4px;border: 4px solid white;background-color:#179871;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b></b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"Forward_propagation\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\" ><h1>Forward propagation<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*XWlcMkDcosYwIAcTO2cCQw.png\" style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/forward_pass_0.gif\">\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/forward_pass_1.gif\">\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/forward_pass_2.gif\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"backward_propagation\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"><h1>backward propagation<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*5A9bmIhGt_Rx8piZ_ybtsg.png\" style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/backpropagation_0.gif\" >\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/backpropagation_1.gif\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/backpropagation_2.gif\">\n",
    "<img src=\"https://github.com/rragundez/PyDataAmsterdam2018/raw/94ba390d7bf5f13c5e8242fb67e9e350785e24e2/images/backpropagation_3.gif\">\n",
    "    </div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/leriomaggio/deep-learning-keras-tensorflow/raw/ee1e0fef293767335ded05ab8ad581c000dcdc1e/imgs/dl_overview.png\" style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"list\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"\">\n",
    "    <h1>list of layers, optimazer funcation and activations funcation </h1> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Activation', 'ActivityRegularization', 'Add', 'AlphaDropout', 'AtrousConv1D', 'AtrousConv2D', 'AtrousConvolution1D', 'AtrousConvolution2D', 'Average', 'AveragePooling1D', 'AveragePooling2D', 'AveragePooling3D', 'AvgPool1D', 'AvgPool2D', 'AvgPool3D', 'BatchNormalization', 'Bidirectional', 'Concatenate', 'Conv1D', 'Conv2D', 'Conv2DTranspose', 'Conv3D', 'Conv3DTranspose', 'ConvLSTM2D', 'ConvLSTM2DCell', 'ConvRNN2D', 'ConvRecurrent2D', 'Convolution1D', 'Convolution2D', 'Convolution2DTranspose', 'Convolution3D', 'Cropping1D', 'Cropping2D', 'Cropping3D', 'CuDNNGRU', 'CuDNNLSTM', 'Deconv2D', 'Deconv3D', 'Deconvolution2D', 'Deconvolution3D', 'Dense', 'DepthwiseConv2D', 'Dot', 'Dropout', 'ELU', 'Embedding', 'Flatten', 'GRU', 'GRUCell', 'GaussianDropout', 'GaussianNoise', 'GlobalAveragePooling1D', 'GlobalAveragePooling2D', 'GlobalAveragePooling3D', 'GlobalAvgPool1D', 'GlobalAvgPool2D', 'GlobalAvgPool3D', 'GlobalMaxPool1D', 'GlobalMaxPool2D', 'GlobalMaxPool3D', 'GlobalMaxPooling1D', 'GlobalMaxPooling2D', 'GlobalMaxPooling3D', 'Highway', 'Input', 'InputLayer', 'InputSpec', 'K', 'LSTM', 'LSTMCell', 'Lambda', 'Layer', 'LeakyReLU', 'LocallyConnected1D', 'LocallyConnected2D', 'Masking', 'MaxPool1D', 'MaxPool2D', 'MaxPool3D', 'MaxPooling1D', 'MaxPooling2D', 'MaxPooling3D', 'Maximum', 'MaxoutDense', 'Minimum', 'Multiply', 'PReLU', 'Permute', 'RNN', 'ReLU', 'Recurrent', 'RepeatVector', 'Reshape', 'SeparableConv1D', 'SeparableConv2D', 'SeparableConvolution1D', 'SeparableConvolution2D', 'SimpleRNN', 'SimpleRNNCell', 'Softmax', 'SpatialDropout1D', 'SpatialDropout2D', 'SpatialDropout3D', 'StackedRNNCells', 'Subtract', 'ThresholdedReLU', 'TimeDistributed', 'UpSampling1D', 'UpSampling2D', 'UpSampling3D', 'Wrapper', 'ZeroPadding1D', 'ZeroPadding2D', 'ZeroPadding3D', 'absolute_import', 'activations', 'add', 'advanced_activations', 'average', 'concatenate', 'constraints', 'conv_utils', 'convolutional', 'convolutional_recurrent', 'copy', 'core', 'cudnn_recurrent', 'deserialize', 'deserialize_keras_object', 'division', 'dot', 'embeddings', 'func_dump', 'func_load', 'has_arg', 'initializers', 'interfaces', 'local', 'maximum', 'merge', 'minimum', 'multiply', 'namedtuple', 'noise', 'normalization', 'np', 'object_list_uid', 'pooling', 'print_function', 'python_types', 'recurrent', 'regularizers', 'serialize', 'subtract', 'to_list', 'transpose_shape', 'warnings', 'wrappers']\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "list_layer = [layer for layer in dir(layers) if not layer.startswith('_')]\n",
    "print(list_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of optimazer funcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Adadelta', 'Adagrad', 'Adam', 'Adamax', 'K', 'Nadam', 'Optimizer', 'RMSprop', 'SGD', 'TFOptimizer', 'absolute_import', 'adadelta', 'adagrad', 'adam', 'adamax', 'clip_norm', 'copy', 'deserialize', 'deserialize_keras_object', 'division', 'get', 'interfaces', 'nadam', 'print_function', 'rmsprop', 'serialize', 'serialize_keras_object', 'sgd', 'six', 'tf', 'zip']\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "list_optimazer = [ opt for opt in dir(optimizers) if not opt.startswith('_')]\n",
    "print(list_optimazer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of activations funcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'Layer', 'absolute_import', 'deserialize', 'deserialize_keras_object', 'division', 'elu', 'exponential', 'get', 'hard_sigmoid', 'linear', 'print_function', 'relu', 'selu', 'serialize', 'sigmoid', 'six', 'softmax', 'softplus', 'softsign', 'tanh', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "from keras import activations\n",
    "\n",
    "list_activations = [ act for act in dir(activations) if not act.startswith('_')]\n",
    "\n",
    "print(list_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list of losses funcation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K', 'KLD', 'MAE', 'MAPE', 'MSE', 'MSLE', 'absolute_import', 'binary_crossentropy', 'categorical_crossentropy', 'categorical_hinge', 'cosine', 'cosine_proximity', 'deserialize', 'deserialize_keras_object', 'division', 'get', 'hinge', 'kld', 'kullback_leibler_divergence', 'logcosh', 'mae', 'mape', 'mean_absolute_error', 'mean_absolute_percentage_error', 'mean_squared_error', 'mean_squared_logarithmic_error', 'mse', 'msle', 'poisson', 'print_function', 'serialize', 'serialize_keras_object', 'six', 'sparse_categorical_crossentropy', 'squared_hinge']\n"
     ]
    }
   ],
   "source": [
    "from keras import losses\n",
    "list_losses = [ losse for losse in dir(losses) if not losse.startswith('_')]\n",
    "print(list_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"layers\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"><h1>Theory of layers<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Activaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BatchNormalization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "orginal page : <a class=\"alert-link\" href=\"https://keras.io/layers/normalization/ \"> https://keras.io/layers/normalization/  </a><br>\n",
    "why should use it: https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c<br>\n",
    "math behind it:  https://www.youtube.com/watch?v=nUUqwaxLnWs   \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros',\n",
    "                                gamma_initializer='ones', moving_mean_initializer='zeros', \n",
    "                                moving_variance_initializer='ones', beta_regularizer=None, \n",
    "                                gamma_regularizer=None, beta_constraint=None, gamma_constraint=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize the input  by adjusting and scaling the activations. For example, when we have features from 0 to 1 and some from 1 to 1000, we should normalize them to speed up learning.<br>\n",
    "Batch normalization reduces the amount by what the hidden unit values shift around (covariance shift)<br>\n",
    "Also, batch normalization allows each layer of a network to learn by itself a little bit more independently of other layers.<br>\n",
    "<b>example:</b><br>\n",
    "To explain covariance shift, let’s have a deep network on cat detection. We train our data on only black cats’ images. So, if we now try to apply this network to data with colored cats, it is obvious; we’re not going to do well. The training set and the prediction set are both cats’ images but they differ a little bit. In other words, if an algorithm learned some X to Y mapping, and if the distribution of X changes, then we might need to retrain the learning algorithm by trying to align the distribution of X with the distribution of Y. \n",
    "\n",
    "-  use higher learning rates because batch normalization makes sure that there’s no activation that’s gone really high or really low\n",
    "- It reduces overfitting because it has a slight regularization effects.\n",
    "-  Similar to dropout, it adds some noise to each hidden layer’s activations. Therefore, if we use batch normalization, we will use less dropout, which is a good thing because we are not going to lose a lot of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "orginal page: https://keras.io/layers/core/ <br>\n",
    "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5  <br>\n",
    "https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/dropout_layer.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dropout(rate, noise_shape=None, seed=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>How it works:</b><br>\n",
    "Basically during training half of neurons on a particular layer will be deactivated. This improve generalization because force your layer to learn with different neurons the same \"concept\".\n",
    "During the prediction phase the dropout is deactivated.\n",
    "\n",
    "\n",
    "<img style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);' src='https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/dropout.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dropout forces a neural network to learn more robust features that are useful in conjunction with many different random subsets of the other neurons.\n",
    "- Dropout roughly doubles the number of iterations required to converge. However, training time for each epoch is less.\n",
    "- With H hidden units, each of which can be dropped, we have 2^H possible models. In testing phase, the entire network is considered and each activation is reduced by a factor p.\n",
    "- it adds some noise to each hidden layer’s activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rate: float between 0 and 1. Fraction of the input units to drop.\n",
    "- noise_shape: 1D integer tensor representing the shape of the binary dropout mask that will be multiplied with the input. For instance, if your inputs have shape  (batch_size, timesteps, features) and you want the dropout mask to be the same for all timesteps, you can use noise_shape=(batch_size, 1, features).\n",
    "- seed: A Python integer to use as random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"optimazer_funcation\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"><h1>Theory of optimazer funcation<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent (SGD) :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/dda3670f8a8996a0d3bf80856bb4a166cc8db6d4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with momentum :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/3959fd86a726cd35d9aeaeaf78ed5006303f7951\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9305a474135c05fa8767aebd66c404e189506d03\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD + Nesterov :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  <mtable columnalign=\"right left right left right left right left right left right left\" rowspacing=\"3pt\" columnspacing=\"0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em\" displaystyle=\"true\">\n",
    "    <mtr>\n",
    "      <mtd>\n",
    "        <mtable columnalign=\"right left\" rowspacing=\"3pt\" columnspacing=\"0em\" displaystyle=\"true\">\n",
    "          <mtr>\n",
    "            <mtd>\n",
    "              <msub>\n",
    "                <mi>v</mi>\n",
    "                <mi>t</mi>\n",
    "              </msub>\n",
    "            </mtd>\n",
    "            <mtd>\n",
    "              <mi></mi>\n",
    "              <mo>=</mo>\n",
    "              <mi>&#x03B3;<!-- γ --></mi>\n",
    "              <msub>\n",
    "                <mi>v</mi>\n",
    "                <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "                  <mi>t</mi>\n",
    "                  <mo>&#x2212;<!-- − --></mo>\n",
    "                  <mn>1</mn>\n",
    "                </mrow>\n",
    "              </msub>\n",
    "              <mo>+</mo>\n",
    "              <mi>&#x03B7;<!-- η --></mi>\n",
    "              <msub>\n",
    "                <mi mathvariant=\"normal\">&#x2207;<!-- ∇ --></mi>\n",
    "                <mi>&#x03B8;<!-- θ --></mi>\n",
    "              </msub>\n",
    "              <mi>J</mi>\n",
    "              <mo stretchy=\"false\">(</mo>\n",
    "              <mi>&#x03B8;<!-- θ --></mi>\n",
    "              <mo>&#x2212;<!-- − --></mo>\n",
    "              <mi>&#x03B3;<!-- γ --></mi>\n",
    "              <msub>\n",
    "                <mi>v</mi>\n",
    "                <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "                  <mi>t</mi>\n",
    "                  <mo>&#x2212;<!-- − --></mo>\n",
    "                  <mn>1</mn>\n",
    "                </mrow>\n",
    "              </msub>\n",
    "              <mo stretchy=\"false\">)</mo>\n",
    "            </mtd>\n",
    "          </mtr>\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mtr>\n",
    "            <mtd>\n",
    "              <mi>&#x03B8;<!-- θ --></mi>\n",
    "            </mtd>\n",
    "            <mtd>\n",
    "              <mi></mi>\n",
    "              <mo>=</mo>\n",
    "              <mi>&#x03B8;<!-- θ --></mi>\n",
    "              <mo>&#x2212;<!-- − --></mo>\n",
    "              <msub>\n",
    "                <mi>v</mi>\n",
    "                <mi>t</mi>\n",
    "              </msub>\n",
    "            </mtd>\n",
    "          </mtr>\n",
    "        </mtable>\n",
    "      </mtd>\n",
    "    </mtr>\n",
    "  </mtable>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGrad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"w3-left\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e388b9155519b8769930b3764f4dadc20eb593b8\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"w3-left\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/034d5652b502094ab7f58f95a383e0ec41de5b77\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"w3-left\"  src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1625bff4ce904cc83c3cadad4bc1a2ff61422b02\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"w3-left\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/7c5ea1207fc3574a51d439f84370a989deffa871\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/abcd4c729bac933249992e086fa1ba7807e1cd09\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div id=\"activations_funcation\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"><h1>Theory of activations funcation<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\n",
    "https://en.wikipedia.org/wiki/Activation_function\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# softmax :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);' src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/5dc2e7b03ea31dd2ad75d939dc93eb0bfc1fa050\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanh:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='border: 4px solid;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);' src='https://cdn-images-1.medium.com/max/1600/1*f9erByySVjTjohfFdNkJYQ.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img class=\"w3-card-4 w3-left\" src='https://wikimedia.org/api/rest_v1/media/math/render/svg/84c428bf21e34ccc0be8becf3443b06a4b61f3ee'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 1px solid white;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'>\n",
    "<header class=\"w3-container w3-center w3-border  w3-round-large\"><h3>sigmoid</h3></header>\n",
    "    \n",
    " <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='border: 1px solid white;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);' src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9537e778e229470d85a68ee0b099c08298a1a3f6\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='left:3px;border: 1px solid white;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);' src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Activation_prelu.svg/240px-Activation_prelu.svg.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='border: 1px solid white;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);' src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/aaabce8985d074b5f4482f4efa327c7c61da3ca6\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"losses_funcation\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"><h1>Theory of losses funcation<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "\n",
    "https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. binary_crossentropy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "https://en.wikipedia.org/wiki/Cross_entropy\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='border: 1px solid white;box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);'  src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is usually used for when we have more  than one class <br>\n",
    "funcation : E[p,q] = ∑p_{i} log(q_{i}) <br>\n",
    "( estimated probability of outcome i is q_{i} )<br>\n",
    "( frequency (empirical probability) of outcome i in the training set p_{i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"losses_funcation\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\"><h1> model fit </h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(x=None, y=None, \n",
    "    batch_size=None, \n",
    "    epochs=1, verbose=1, \n",
    "    callbacks=None, \n",
    "    validation_split=0.0, \n",
    "    validation_data=None, \n",
    "    shuffle=True, \n",
    "    class_weight=None, \n",
    "    sample_weight=None, \n",
    "    initial_epoch=0, \n",
    "    steps_per_epoch=None, \n",
    "    validation_steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments:\n",
    "\n",
    "x: Numpy array of training data (if the model has a single input), or list of Numpy arrays (if the model has multiple inputs). If input layers in the model are named, you can also pass a dictionary mapping input names to Numpy arrays.  x can be None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors).\n",
    "\n",
    "y: Numpy array of target (label) data (if the model has a single output), or list of Numpy arrays (if the model has multiple outputs). If output layers in the model are named, you can also pass a dictionary mapping output names to Numpy arrays.  y can be None (default) if feeding from framework-native tensors (e.g. TensorFlow data tensors).\n",
    "\n",
    "<b>batch_size:</b> Integer or None. Number of samples per gradient update. If unspecified, batch_size will default to 32.\n",
    "epochs: Integer. Number of epochs to train the model. An epoch is an iteration over the entire x and y data provided. Note that in conjunction with initial_epoch,  epochs is to be understood as \"final epoch\". The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached.\n",
    "- For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. Next it takes the second 100 samples (from 101st to 200th) and trains the network again. We can keep doing this procedure until we have propagated through all samples of the network. A problem usually happens with the last set of samples. In our example we've used 1050 which is not divisible by 100 without remainder. The simplest solution is just to get the final 50 samples and train the network.\n",
    "\n",
    "verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "callbacks: List of keras.callbacks.Callback instances. List of callbacks to apply during training. See callbacks.\n",
    "\n",
    "validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling.\n",
    "\n",
    "validation_data: tuple (x_val, y_val) or tuple  (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data.  validation_data will override validation_split.\n",
    "\n",
    "shuffle: Boolean (whether to shuffle the training data before each epoch) or str (for 'batch'). 'batch' is a special option for dealing with the limitations of HDF5 data; it shuffles in batch-sized chunks. Has no effect when steps_per_epoch is not None.\n",
    "\n",
    "class_weight: Optional dictionary mapping class indices (integers) to a weight (float) value, used for weighting the loss function (during training only). This can be useful to tell the model to \"pay more attention\" to samples from an under-represented class.\n",
    "\n",
    "sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). You can either pass a flat (1D) Numpy array with the same length as the input samples (1:1 mapping between weights and samples), or in the case of temporal data, you can pass a 2D array with shape  (samples, sequence_length), to apply a different weight to every timestep of every sample. In this case you should make sure to specify  sample_weight_mode=\"temporal\" in compile().\n",
    "\n",
    "initial_epoch: Integer. Epoch at which to start training (useful for resuming a previous training run).\n",
    "steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.\n",
    "\n",
    "validation_steps: Only relevant if steps_per_epoch is specified. Total number of steps (batches of samples) to validate before stopping.\n",
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "https://keras.io/models/model/\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
