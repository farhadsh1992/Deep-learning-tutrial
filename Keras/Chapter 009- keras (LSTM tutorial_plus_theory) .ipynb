{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:125px;text-align:center;border: 4px solid #02541B;background-color:#02541B;color:white\">\n",
    "\n",
    "<header  style=\"width:100%;height:100px;\">\n",
    "  <h1><b>Chapter 9</b></h1>\n",
    "    <h4>Illustrated Guide to LSTM’s and GRU’s: A step by step explanation,Plus Theory</h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brief contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div  style=\"width:300px;position:absolute;left: auto;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#plus\" style=\"padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Plus Theory</b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:300px;position:absolute;left: 305px;border: 4px solid white;background-color:#02541B;color:whitee\">\n",
    "    <header></header>\n",
    "    <a href=\"#GL\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>LSTM’s and GRU’s</b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 610px;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#review\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Review of Recurrent Neural Networks </b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 915px;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#lstm\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>(LSTM) LONG_SHORT TERM MEMORY</b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 1220px;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>Code Demo</b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!------------------------------------------------------------------------------------------------------------------------>\n",
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div  style=\"width:300px;position:absolute;left: auto;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#gru\" style=\"padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b>GRU</b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:300px;position:absolute;left: 305px;border: 4px solid white;background-color:#02541B;color:whitee\">\n",
    "    <header></header>\n",
    "    <a href=\"#\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b></b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 610px;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b> </b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 915px;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b></b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 1220px;border: 4px solid white;background-color:#02541B;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#\"style=\"position: relative;padding:5px;color:white;text-align: center;\" href=\"#Chapter3\">\n",
    "      <h4 ><b></b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!------------------------------------------------------------------------------------------------------------------------>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='plus' style=\"width:100%;height:70px;border: 4px solid #02541B;background-color:#02541B;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Plus Theory</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "    \n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "    \n",
    "<b>Important:</b>\n",
    "    \n",
    "https://www.getrevue.co/profile/wildml\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=WCUNPb-5EYI\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> new.wieght = old.wieght - learning rate*gradient\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GL' style=\"width:100%;height:70px;border: 4px solid #02541B;background-color:#02541B;;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>LSTM’s and GRU’s</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*yBXV9o5q7L_CvY7quJt3WQ.png\" width=\"500\" ></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an LSTM or GRU It can learn to keep only relevant information to make predictions, and forget non relevant data.<br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ygAgowqTZjR6ABzZHd8Bqg.gif\" width=\"500\" ></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='review' style=\"width:100%;height:70px;border: 4px solid #02541B;background-color:#02541B;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Review of Recurrent Neural Networks </b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First words get transformed into machine-readable vectors. Then the RNN processes the sequence of vectors one by one. While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.\n",
    "<br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*AQ52bwW55GsJt6HTxPDuMA.gif\" style=\"wide:100px\"></img>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif\" width=\"500\" ></img>\n",
    "\n",
    "\n",
    "First, the input and previous hidden state are combined to form a vector. That vector now has information on the current input and previous inputs. The vector goes through the tanh activation, and the output is the new hidden state, or the memory of the network.<br>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*WMnFSJHzOloFlJHU6fVN-g.gif\" width=\"400\" ></img>\n",
    "\n",
    "### Tanh activation: \n",
    "he tanh activation is used to help regulate the values flowing through the network. The tanh function squishes values to always be between -1 and 1.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*iRlEg1GBKRzGTre5aOQUCg.gif\" width=\"300\" ></img>\n",
    "\n",
    " So imagine a value that continues to be multiplied by let’s say 3. You can see how some values can explode and become astronomical, causing other values to seem insignificant.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*LgbEFcGiUpseZ--M7wuZhg.gif\" width=\"300\" ></img>\n",
    "\n",
    "A tanh function ensures that the values stay between -1 and 1, thus regulating the output of the neural network. You can see how the same values from above remain between the boundaries allowed by the tanh function.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*gFC2bTg3uihp1klknWU0qg.gif\" width=\"300\" ></img>\n",
    "\n",
    "that’s an RNN. It has very few operations internally but works pretty well given the right circumstances (like short sequences). RNN’s uses a lot less computational resources than it’s evolved variants, LSTM’s and GRU’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='lstm' style=\"width:100%;height:70px;border: 4px solid #02541B;background-color:#02541B;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>(LSTM) LONG_SHORT TERM MEMORY</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*0f8r3Vd-i4ueYND1CUrhMA.png\" width=\"360\" style=\"float:left\"></img><img src=\"https://cdn-images-1.medium.com/max/1600/1*yBXV9o5q7L_CvY7quJt3WQ.png\" width=\"400\" style=\"float:right\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sigmoid Activation :</b><br>\n",
    "A sigmoid activation is similar to the tanh activation. Instead of squishing values between -1 and 1,  it squishes values between 0 and 1.That is helpful to update or forget data because any number getting multiplied by 0 is 0. causing values to disappears or be “forgotten.” Any number multiplied by 1 is the same value therefore that value stay’s the same or is “kept.\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*rOFozAke2DX5BmsX2ubovw.gif\" width=\"360\" ></img><br>\n",
    "\n",
    "<b>The cell state :</b><br>\n",
    "act as a transport highway that transfers relative information all the way down the sequence chain.  it is as the “memory” of the network. cell state goes on its journey, information get’s added or removed to the cell state via gates.The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.<br>\n",
    "First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. <br>\n",
    "Secound, we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. <br>\n",
    "Finally, That gives us our new cell state.<br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*S0rXIeO_VoUVOyrYHckUWg.gif\" width=\"360\" style=\"float:left\"></img><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have <b>three different gates</b> that regulate information flow in an LSTM cell:<br> \n",
    "- A forget gate, \n",
    "- input gate, \n",
    "- output gate.<br>\n",
    "the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be.<br>\n",
    "\n",
    "\n",
    "<b>Forget gate:</b><br>\n",
    " 0 means to forget<br>\n",
    " 1 means to keep.<br>\n",
    " \n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*GjehOa513_BgpDDP6Vkw2Q.gif\" width=\"360\" ></img><br>\n",
    "\n",
    "<b>Input Gate:</b><br>\n",
    "First, we pass the previous hidden state and current input into a sigmoid function.<br>\n",
    "Secound, You pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. <br>\n",
    "Third, you multiply the tanh output with the sigmoid output.The sigmoid output will decide which information is important to keep from the tanh output.<br>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*TTmYy7Sy8uUXxUXfzmoKbA.gif\" width=\"360\" ></img><br>\n",
    "\n",
    "<b>Output Gate:</b><br>\n",
    "The output gate decides what the next hidden state should be.Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions.<br>\n",
    "First, we pass the previous hidden state and the current input into a sigmoid function.<br>\n",
    "Secound, we pass the newly modified cell state to the tanh function.<br>\n",
    "Third, We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. <br>\n",
    "finally, The new cell state and the new hidden is then carried over to the next time step.<br>\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*VOXRGhOShoWWks6ouoDN3Q.gif\" width=\"360\" style=\"float:left\"></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='code' style=\"width:100%;height:70px;border: 4px solid #02541B;background-color:#02541B;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Code Demo:</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_ct => gate up that go ditrectly for cell state\n",
    "#prev_ht => gate down that add with new input data.\n",
    "\n",
    "def LSTM(prev_ct, prev_ht, Input):\n",
    "    \n",
    "    # previous hidden state and the current input get concatenated. \n",
    "    combine = prev_ht+ Input\n",
    "    \n",
    "    # Combine get’s fed into the forget layer. This layer removes non-relevant data.\n",
    "    ft = forget_layer(combie)\n",
    "    \n",
    "    # A candidate layer is created using combine. \n",
    "    #The candidate holds possible values to add to the cell state.\n",
    "    condidate = condidate_layer(combine)\n",
    "    \n",
    "    # Combine also get’s fed into the input layer. \n",
    "    # This layer decides what data from the candidate should be added to the new cell state.\n",
    "    it = input_layer(combine)\n",
    "    \n",
    "    # the cell state is calculated using those vectors and the previous cell state.\n",
    "    ct =  prev_ct * ft + condidate *it\n",
    "    \n",
    "    # The output is then computed.\n",
    "    ot = output_layer(combine)\n",
    "    \n",
    "    #  Pointwise multiplying the output and the new cell state gives us the new hidden state.\n",
    "    ht = ot* tanh(ct)\n",
    "    return ht, ct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = []\n",
    "ht = []\n",
    "for Input as inputs:\n",
    "    ct,ht = LSTMCELL(ct, ht, Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='GRU' style=\"width:100%;height:70px;border: 4px solid #02541B;background-color:#02541B;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>GRU</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*jhi5uOm9PvZfmxvfaCektw.png\" width=\"410\" ></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates:\n",
    "- a reset gate, \n",
    "- update gate.<br>\n",
    "\n",
    "<b>Update Gate:</b><br>\n",
    "The update gate acts similar to the forget and input gate of an LSTM. It decides what information to throw away and what new information to add.<br>\n",
    "\n",
    "<b>Reset Gate:</b><br>\n",
    "The reset gate is another gate is used to decide how much past information to forget.<br>\n",
    " GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s. <br><br>\n",
    " \n",
    "Researchers and engineers usually try both to determine which one works better for their use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
