{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:125px;text-align:center;border: 4px solid #660033;background-color:#660033;color:white\">\n",
    "\n",
    "<header  style=\"width:100%;height:100px;\">\n",
    "  <h1><b>Chapter 10</b></h1>\n",
    "    <h4>Chat Bots & NLP for Deep Learning</h4>\n",
    "</header>\n",
    "\n",
    "<div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='border: 4px solid #660033;padding:9px;'>\n",
    "\n",
    "By: Farhad Shadmand \n",
    "    \n",
    "https://github.com/farhadsh1992\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# brief contents:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div  style=\"width:300px;position:absolute;left: auto;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#chat\" style=\"padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>Chat Bots & NLP</b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:300px;position:absolute;left: 305px;border: 4px solid white;background-color:#660033;color:whitee\">\n",
    "    <header></header>\n",
    "    <a href=\"#nlp\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h5 ><b>NLP & End to End Network in Keras and TensorFlow </b></h5>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 610px;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#end\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>End-to-End Memory Network.</b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 915px;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#data\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>load data</b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 1220px;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#building\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>Building the Vocabulary</b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!------------------------------------------------------------------------------------------------------------------------>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"position: relative;height:100px;\">\n",
    "    \n",
    "<div  style=\"width:300px;position:absolute;left: auto;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#building_T\" style=\"padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>Building the Training and Test Data</b></h4>\n",
    "      </a>\n",
    " </div>\n",
    "    \n",
    " <div style=\"width:300px;position:absolute;left: 305px;border: 4px solid white;background-color:#660033;color:whitee\">\n",
    "    <header></header>\n",
    "    <a href=\"#compile\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>Compile the Neural Network</b></h4>\n",
    "      </a>\n",
    "     \n",
    "  </div>\n",
    "    <div style=\"width:300px;position:absolute;left: 610px;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"plus\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b>Plus Theory</b></h4>\n",
    "      </a>\n",
    "        </div>\n",
    "\n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 915px;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b></b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "  \n",
    "    \n",
    "   <div  style=\"width:300px;position:absolute;left: 1220px;border: 4px solid white;background-color:#660033;color:white\">\n",
    "    <header></header>\n",
    "    <a href=\"#list\"style=\"position: relative;padding:5px;color:white;text-align: center;\" >\n",
    "      <h4 ><b></b></h4>\n",
    "      </a>\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<!------------------------------------------------------------------------------------------------------------------------>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"chat\" style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Chat Bots & NLP for Deep Learning in TensorFlow and Keras</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "\n",
    "\n",
    "https://www.youtube.com/watch?v=bv_iVVrlfbU&index=37&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN&frags=wn\n",
    "    \n",
    "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class11_text_nlp.ipynb    \n",
    "    \n",
    "\n",
    "Facabook: https://research.fb.com/downloads/babi/\n",
    "\n",
    "https://arxiv.org/abs/1502.05698\n",
    "\n",
    "<b> https://arxiv.org/abs/1503.08895 </b>\n",
    "\n",
    "https://pdos.csail.mit.edu/archive/scigen/\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='nlp' style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>NLP & End to End Network in Keras and TensorFlow </b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The origional source papers for End-to-End Memory Networks:\n",
    "Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush, [\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"](http://arxiv.org/abs/1502.05698) <br>\n",
    "Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, [\"End-To-End Memory Networks\"](http://arxiv.org/abs/1503.08895)\n",
    "\n",
    "\n",
    "other:\n",
    "- [Keras End-To-End Memory Networks](https://github.com/fchollet/keras/blob/master/examples/babi_memnn.py)\n",
    "\n",
    "-[bAbI Datasets](https://research.fb.com/downloads/babi/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='end' style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>End-to-End Memory Network.</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_jefi(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "    If only_supporting is true, only the sentences\n",
    "    that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = word_tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = word_tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''\n",
    "    Given a file name, read the file,\n",
    "    retrieve the stories,\n",
    "    and then convert the sentences into a single story.\n",
    "    If max_length is supplied,\n",
    "    any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for \n",
    "            story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nApply function of two arguments cumulatively to the items of sequence,\\nfrom left to right, so as to reduce the sequence to a single value\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.data_utils  import get_file\n",
    "\n",
    "import tarfile # open tar file\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from functools import reduce\n",
    "\"\"\"\n",
    "Apply function of two arguments cumulatively to the items of sequence,\n",
    "from left to right, so as to reduce the sequence to a single value\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='data' style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>get data</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting stories for the challenge: single_supporting_fact_10k\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    path = get_file('babi-tasks-v1-2.tar.get', \n",
    "                    origin='https://s3.amazonaws.com/text-datasets/babi_tasks_1-20_v1-2.tar.gz')\n",
    "except:\n",
    "    print('Error downloading dataset, please download it manually:\\n'\n",
    "          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\\n'\n",
    "          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n",
    "    raise\n",
    "tar = tarfile.open(path)    \n",
    "\n",
    "\n",
    "challenges = {\n",
    "    # QA1 with 10,000 samples\n",
    "    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt',\n",
    "    # QA2 with 10,000 samples\n",
    "    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt',\n",
    "}\n",
    "challenge_type = 'single_supporting_fact_10k'\n",
    "challenge = challenges[challenge_type]\n",
    "\n",
    "print('Extracting stories for the challenge:', challenge_type)\n",
    "train_stories = get_stories(tar.extractfile(challenge.format('train')))\n",
    "test_stories = get_stories(tar.extractfile(challenge.format('test')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where', 'is', 'Mary', '?']\n",
      "Story: Mary moved to the bathroom . John went to the hallway .\n",
      "Query: Where is Mary ?\n",
      "Answer: bathroom\n",
      "---\n",
      "Story: Mary moved to the bathroom . John went to the hallway . Daniel went back to the hallway . Sandra moved to the garden .\n",
      "Query: Where is Daniel ?\n",
      "Answer: hallway\n",
      "---\n",
      "Story: Mary moved to the bathroom . John went to the hallway . Daniel went back to the hallway . Sandra moved to the garden . John moved to the office . Sandra journeyed to the bathroom .\n",
      "Query: Where is Daniel ?\n",
      "Answer: hallway\n",
      "---\n",
      "Story: Mary moved to the bathroom . John went to the hallway . Daniel went back to the hallway . Sandra moved to the garden . John moved to the office . Sandra journeyed to the bathroom . Mary moved to the hallway . Daniel travelled to the office .\n",
      "Query: Where is Daniel ?\n",
      "Answer: office\n",
      "---\n",
      "Story: Mary moved to the bathroom . John went to the hallway . Daniel went back to the hallway . Sandra moved to the garden . John moved to the office . Sandra journeyed to the bathroom . Mary moved to the hallway . Daniel travelled to the office . John went back to the garden . John moved to the bedroom .\n",
      "Query: Where is Sandra ?\n",
      "Answer: bathroom\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# See what the data looks like\n",
    "print(train_stories[0][1])\n",
    "for i in range(5):\n",
    "    print(\"Story: {}\".format(' '.join(train_stories[i][0])))\n",
    "    print(\"Query: {}\".format(' '.join(train_stories[i][1])))\n",
    "    print(\"Answer: {}\".format(train_stories[i][2]))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='building'style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Building the Vocabulary</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Vocab size: 22 unique words\n",
      "Story max length: 68 words\n",
      "Query max length: 4 words\n",
      "Number of training stories: 10000\n",
      "Number of test stories: 1000\n",
      "-\n",
      "Here's what a \"story\" tuple looks like (input, query, answer):\n",
      "(['Mary', 'moved', 'to', 'the', 'bathroom', '.', 'John', 'went', 'to', 'the', 'hallway', '.'], ['Where', 'is', 'Mary', '?'], 'bathroom')\n",
      "-\n",
      "(0, '.')\n",
      "(1, '?')\n",
      "(2, 'Daniel')\n",
      "(3, 'John')\n",
      "(4, 'Mary')\n",
      "(5, 'Sandra')\n",
      "(6, 'Where')\n",
      "(7, 'back')\n",
      "(8, 'bathroom')\n",
      "(9, 'bedroom')\n",
      "(10, 'garden')\n",
      "(11, 'hallway')\n",
      "(12, 'is')\n",
      "(13, 'journeyed')\n",
      "(14, 'kitchen')\n",
      "(15, 'moved')\n",
      "(16, 'office')\n",
      "(17, 'the')\n",
      "(18, 'to')\n",
      "(19, 'travelled')\n",
      "(20, 'went')\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for story, q, answer in train_stories + test_stories:\n",
    "    vocab |= set(story + q + [answer])\n",
    "vocab = sorted(vocab)\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n",
    "\n",
    "print('-')\n",
    "print('Vocab size:', vocab_size, 'unique words')\n",
    "print('Story max length:', story_maxlen, 'words')\n",
    "print('Query max length:', query_maxlen, 'words')\n",
    "print('Number of training stories:', len(train_stories))\n",
    "print('Number of test stories:', len(test_stories))\n",
    "print('-')\n",
    "print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "print(train_stories[0])\n",
    "print('-')\n",
    "\n",
    "\n",
    "for s in list(enumerate(vocab)):\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='building_T' style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Building the Training and Test Data</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "def vectorize_stories(data):\n",
    "    inputs, queries, answers = [], [], []\n",
    "    for story, query, answer in data:\n",
    "        inputs.append([word_idx[w] for w in story])\n",
    "        queries.append([word_idx[w] for w in query])\n",
    "        answers.append(word_idx[answer])\n",
    "    return (pad_sequences(inputs, maxlen=story_maxlen),\n",
    "            pad_sequences(queries, maxlen=query_maxlen),\n",
    "            np.array(answers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing the word sequences...\n",
      "-\n",
      "inputs: integer tensor of shape (samples, max_length)\n",
      "inputs_train shape: (10000, 68)\n",
      "inputs_test shape: (1000, 68)\n",
      "-\n",
      "queries: integer tensor of shape (samples, max_length)\n",
      "queries_train shape: (10000, 4)\n",
      "queries_test shape: (1000, 4)\n",
      "-\n",
      "answers: binary (1 or 0) tensor of shape (samples, vocab_size)\n",
      "answers_train shape: (10000,)\n",
      "answers_test shape: (1000,)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "print('Vectorizing the word sequences...')\n",
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n",
    "inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n",
    "\n",
    "print('-')\n",
    "print('inputs: integer tensor of shape (samples, max_length)')\n",
    "print('inputs_train shape:', inputs_train.shape)\n",
    "print('inputs_test shape:', inputs_test.shape)\n",
    "print('-')\n",
    "print('queries: integer tensor of shape (samples, max_length)')\n",
    "print('queries_train shape:', queries_train.shape)\n",
    "print('queries_test shape:', queries_test.shape)\n",
    "print('-')\n",
    "print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n",
    "print('answers_train shape:', answers_train.shape)\n",
    "print('answers_test shape:', answers_test.shape)\n",
    "print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story (x): [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  5 16 19 18  9  1  4 21 19 18 12  1]\n",
      "Question (x): [ 7 13  5  2]\n",
      "Answer: 9\n"
     ]
    }
   ],
   "source": [
    "# See individual training element.\n",
    "\n",
    "print(\"Story (x): {}\".format(inputs_train[0]))\n",
    "print(\"Question (x): {}\".format(queries_train[0]))\n",
    "print(\"Answer: {}\".format(answers_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='compile' style=\"width:100%;height:70px;border: 4px solid #660033;background-color:#660033;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "  <header><h2><b>Compile the Neural Network</b></h2></header>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Input, Dropout, Activation, LSTM\n",
    "from keras.layers import Embedding, Permute, concatenate  # not Concatentate\n",
    "from keras.layers import dot, add\n",
    "\n",
    "from keras.models import Model\n",
    "#from keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_m():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=64))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_c():\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=query_maxlen))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    return model\n",
    "def model_encoder():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=64, input_length=query_maxlen))\n",
    "    model.add(Dropout(rate=0.3))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_answer(input_sequence,question):\n",
    "  \n",
    "    # encode input sequence and questions (which are indices)\n",
    "    # to sequences of dense vectors\n",
    "    input_encoded_m = model_m()\n",
    "    input_encoded_c = model_c()\n",
    "    question_encoded = model_encoder()\n",
    "    \n",
    "    input_encoded_m = input_encoded_m(input_sequence)\n",
    "    input_encoded_c = input_encoded_c(input_sequence)\n",
    "    question_encoded = question_encoded(question)\n",
    "    \n",
    "    \n",
    "    match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n",
    "    match = Activation('softmax')(match)\n",
    "    \n",
    "    response = add([match, input_encoded_c])  \n",
    "    response = Permute((2, 1))(response)  #\n",
    "\n",
    "    answer = concatenate([response, question_encoded])\n",
    "\n",
    "\n",
    "    answer = LSTM(32)(answer) \n",
    "\n",
    "    answer = Dropout(0.3)(answer)\n",
    "    answer = Dense(vocab_size)(answer)\n",
    "    answer = Activation('softmax')(answer)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def model_where(answer,input_sequence, question):\n",
    "    model2 = Model([input_sequence, question], answer)\n",
    "    #model.compile(loss='sparse_categorical_crossentropy', optimzer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "input_sequence = Input((story_maxlen,))\n",
    "question = Input((query_maxlen,))\n",
    "\n",
    "%time\n",
    "answer = model_answer(input_sequence,question)\n",
    "model = model_where(answer, input_sequence, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"layers\" style=\"width:100%;height:70px;border: 4px solid #179871;background-color:#179871;color:white;text-align:center;border-radius: 25px;padding:3px\">\n",
    "    <h1>+PLus Theory<h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functools.reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = (lambda x, y: x+y, [1, 2, 3, 4, 5])\n",
    "x2 = reduce(lambda x, y: x+y, [1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: (<function <lambda> at 0x1339b1f28>, [1, 2, 3, 4, 5])\n",
      "x2: 15\n"
     ]
    }
   ],
   "source": [
    "print(\"x1: {}\".format(x1))\n",
    "print(\"x2: {}\".format(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layes import <font color='red'>dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #BFE6FF;background-color:#BFE6FF;color:black;border-radius: 5px;padding:7px\">\n",
    "  <strong> Refrence: </strong><br>\n",
    "\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dot\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer that computes a dot product between samples in two tensors.\n",
    "\n",
    "E.g. if applied to a list of two tensors a and b of shape (batch_size, n), the output will be a tensor of shape (batch_size, 1) where each entry i will be the dot product between a[i] and b[i].\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- inputs: A list of input tensors (at least 2).\n",
    "- axes: Integer or tuple of integers, axis or axes along which to take the dot product.\n",
    "- normalize: Whether to L2-normalize samples along the dot product axis before taking the dot product. If set to True, then the output of the dot product is the cosine proximity between the two samples.\n",
    "- **kwargs: Standard layer keyword arguments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layer import <font color='red'> add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional interface to the Add layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layer import <font color='red'>Input, Inputlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input is used to instantiate a Keras tensor.<br>\n",
    "Inputlayer Layer to be used as an entry point into a Network (a graph of layers).\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- input_shape: Shape tuple (not including the batch axis), or TensorShape instance (not including the batch axis).\n",
    "- batch_size: Optional input batch size (integer or None).\n",
    "- dtype: Datatype of the input.\n",
    "- input_tensor: Optional tensor to use as layer input instead of creating a placeholder.\n",
    "- sparse: Boolean, whether the placeholder created is meant to be sparse.\n",
    "- name: Name of the layer (string)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layer import <font color='red'> Permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Permute((2, 1), input_shape=(10, 64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permutes the dimensions of the input according to a given pattern.\n",
    "\n",
    "Useful for e.g. connecting RNNs and convnets together.\n",
    "\n",
    "<b>Output shape:</b>\n",
    "\n",
    "- Same as the input shape, but with the dimensions re-ordered according to the specified pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layer import <font color='red'> concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional interface to the Concatenate layer.\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- inputs: A list of input tensors (at least 2).\n",
    "- axis: Concatenation axis.\n",
    "- **kwargs: Standard layer keyword arguments.\n",
    "\n",
    "Returns:\n",
    "\n",
    "- A tensor, the concatenation of the inputs alongside axis axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layer import <font color='red'> Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "<b>This layer can only be used as the first layer in a model.</b>\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- **input_dim:** int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "- **output_dim:** int >= 0. Dimension of the dense embedding.\n",
    "- **embeddings_initializer:** Initializer for the embeddings matrix.\n",
    "- **embeddings_regularizer:** Regularizer function applied to the embeddings matrix.\n",
    "- **embeddings_constraint:** Constraint function applied to the embeddings matrix.\n",
    "- **mask_zero:** Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when using recurrent layers which may take variable length input. If this is True then - all subsequent layers in the model need to support masking or an exception will be raised. If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal size of vocabulary + 1).\n",
    "- **input_length:** Length of input sequences, when it is constant. This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
